{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import boto3\n",
    "\n",
    "# using this tutorial \n",
    "# https://www.tensorflow.org/tutorials/keras/text_classification_with_hub\n",
    "\n",
    "# uses tf.keras, a high-level API to build and train models in TensorFlow, and TensorFlow Hub, a library and platform for transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Version:  2.0.0\nEager mode:  True\nHub version:  0.7.0\nGPU is NOT AVAILABLE\n"
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "!pip install -q tensorflow-hub\n",
    "!pip install -q tensorflow-datasets\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"Hub version: \", hub.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \n",
    "data = pd.read_csv(\"data/train.csv\")\n",
    "data = data[['target', 'question_text']]\n",
    "target = data.pop(\"target\")\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.33, random_state=42)\n",
    "\n",
    "# using CV instead of static train/test split\n",
    "# https://stackoverflow.com/questions/39748660/how-to-perform-k-fold-cross-validation-with-tensorflow\n",
    "\n",
    "\n",
    "# load to tf.data --> train\n",
    "train_data = tf.data.Dataset.from_tensor_slices((X_train.values, y_train.values))\n",
    "\n",
    "# load to tf.data --> validation\n",
    "validation_data = tf.data.Dataset.from_tensor_slices((X_test.values, y_test.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=1071, shape=(10,), dtype=string, numpy=\narray([b'What are the differences between the Indian work culture and the UK work culture?',\n       b'Is this English sentence correct and not weird?',\n       b'How much of a \"leg up\" would the Cornell human ecology be for a student? Worth the amount of money it would cost?',\n       b'Will Trump get away with all of his lies and blatantly corrupt behavior?',\n       b'What is the meaning of freedom of speech for RSS & BJP person?',\n       b'What is the altitude of Genting Highlands?',\n       b'Is GPA useful for getting better universities through GRE?',\n       b'Can you compare Pakistani with pigs?',\n       b'How pashmina shawl is formed?', b'How is murder morally wrong?'],\n      dtype=object)>"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_examples_batch, train_labels_batch = next(iter(train_data.batch(10)))\n",
    "\n",
    "# reshape \n",
    "train_examples_batch = tf.reshape(train_examples_batch, [10,])\n",
    "\n",
    "# batches --> note the shape\n",
    "train_examples_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=1069, shape=(10,), dtype=int64, numpy=array([0, 0, 0, 0, 1, 0, 0, 1, 0, 0])>"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels\n",
    "train_labels_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "Build the model\n",
    "\n",
    "three main architecture decisions:\n",
    "1) how to represent the data (the text)\n",
    "2) how many layers to use in the model \n",
    "3) how many **hidden** units to use for each layer \n",
    "\n",
    "Transfer Learning \n",
    "One way to represent the text is to convert sentences into embeddings vectors. We can use a pre-trained text embedding as the first layer, which will have three advantages:\n",
    "\n",
    "1) we don't have to worry about text preprocessing,\n",
    "2) we can benefit from transfer learning,\n",
    "3) the embedding has a fixed size, so it's simpler to process.\n",
    "\n",
    "https://blog.fastforwardlabs.com/2019/09/05/transfer-learning-from-the-ground-up.html\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "For this example we will use a pre-trained text embedding model from TensorFlow Hub called google/tf2-preview/gnews-swivel-20dim/1.\n",
    "\n",
    "There are three other pre-trained models to test for the sake of this tutorial:\n",
    "\n",
    "1) google/tf2-preview/gnews-swivel-20dim-with-oov/1 - same as google/tf2-preview/gnews-swivel-20dim/1, but with 2.5% vocabulary converted to OOV buckets. This can help if vocabulary of the task and vocabulary of the model don't fully overlap.\n",
    "\n",
    "2) google/tf2-preview/nnlm-en-dim50/1 - A much larger model with ~1M vocabulary size and 50 dimensions.\n",
    "\n",
    "3) google/tf2-preview/nnlm-en-dim128/1 - Even larger model with ~1M vocabulary size and 128 dimensions.\n",
    "\n",
    "Let's first create a Keras layer that uses a TensorFlow Hub model to embed the sentences, and try it out on a couple of input examples. \n",
    "\n",
    "Note that no matter the length of the input text, the output shape of the embeddings is: (num_examples, embedding_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
    "hub_layer = hub.KerasLayer(embedding, input_shape=[], \n",
    "                           dtype=tf.string, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=1340, shape=(1, 20), dtype=float32, numpy=\narray([[ 1.3658514 ,  1.2020671 ,  2.1172225 , -1.238663  , -0.13164836,\n        -0.79073864,  0.85393584, -0.27051654, -0.028162  , -1.0663986 ,\n         0.68132824, -1.1612318 , -1.7176561 ,  0.5895182 , -1.7590446 ,\n        -0.3582189 ,  1.6168668 ,  0.21547578, -0.8037966 , -0.29350185]],\n      dtype=float32)>"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# projection as the average of embedding dimensions?\n",
    "# example --> 'What are the differences between the Indian work culture and the UK work culture?'\n",
    "hub_layer(train_examples_batch[:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nkeras_layer_4 (KerasLayer)   (None, 20)                400020    \n_________________________________________________________________\ndense (Dense)                (None, 16)                336       \n_________________________________________________________________\ndense_1 (Dense)              (None, 1)                 17        \n=================================================================\nTotal params: 400,373\nTrainable params: 400,373\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "# building full model \n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(hub_layer)\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}