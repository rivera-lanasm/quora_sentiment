{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36964bitf2cd31bfb32c4c9ea03831980e0b0932",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import numpy as np\n",
    "\n",
    "# sklearn \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# import tensorflow_datasets as tfds\n",
    "\n",
    "# using this tutorial \n",
    "# https://www.tensorflow.org/tutorials/keras/text_classification_with_hub\n",
    "# https://www.tensorflow.org/tutorials/structured_data/imbalanced_data\n",
    "\n",
    "# uses tf.keras, a high-level API to build and train models in TensorFlow, and TensorFlow Hub, a library and platform for transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Version:  2.0.0\nEager mode:  True\nHub version:  0.7.0\nGPU is NOT AVAILABLE\n"
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# !pip install -q tensorflow-hub\n",
    "# !pip install -q tensorflow-datasets\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"Hub version: \", hub.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"quora-questions\"\n",
    "file_name = \"data/train.csv\"\n",
    "\n",
    "s3 = boto3.client('s3') \n",
    "# 's3' is a key word. create connection to S3 using default config and all buckets within S3\n",
    "\n",
    "obj = s3.get_object(Bucket= bucket, Key= file_name) \n",
    "# get object and file (key) from bucket\n",
    "data = pd.read_csv(obj['Body'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load data \n",
    "# data = pd.read_csv(\"~/projects/quora_data/train.csv\")\n",
    "data = data[['target', 'question_text']]\n",
    "# target = data.pop(\"target\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Examples:\n    Total: 1306122\n    Positive: 80810 (6.19% of total)\n\n"
    }
   ],
   "source": [
    "neg, pos = np.bincount(target)\n",
    "total = neg + pos\n",
    "print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(\n",
    "    total, pos, 100 * pos / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a utility from sklearn to split and shuffle our dataset\n",
    "train_df, test_df = train_test_split(data, test_size=0.2, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Form np arrays of labels\n",
    "train_labels = np.array(train_df.pop('target'))\n",
    "val_labels = np.array(val_df.pop('target'))\n",
    "test_labels = np.array(test_df.pop('target'))\n",
    "\n",
    "# Form np arrays of features.\n",
    "train_features = np.array(train_df).reshape((len(train_features,)))\n",
    "val_features = np.array(val_df).reshape((len(val_features,)))\n",
    "test_features = np.array(test_df).reshape((len(test_features,)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load to tf.data --> train\n",
    "train_data = tf.data.Dataset.from_tensor_slices((train_features,train_labels))\n",
    "\n",
    "# load to tf.data --> validation\n",
    "validation_data = tf.data.Dataset.from_tensor_slices((val_features, val_labels))\n",
    "\n",
    "# load to tf.data --> test\n",
    "test_data = tf.data.Dataset.from_tensor_slices((test_features, test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([0, 0, 0, ..., 0, 0, 0])"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=45, shape=(10,), dtype=string, numpy=\narray([b'How do I heal from being used by my ex for sex?',\n       b'How do I stop giving a fuck about stuff?',\n       b\"Is it normal if I don't want to see my girlfriend sometimes?\",\n       b'What should I keep in mind when planning to move over to Mazatl\\xc3\\xa1n, Sinaloa?',\n       b'Where is the highest age of consent for sexual relations?',\n       b'Where can I get my cock sucked?',\n       b'How do I prove: sinA/ (1+cosA) =cosecA-cotA?',\n       b\"Has parenting really gotten so bad, that parents turn a blind eye to the child's negative capabilities? Is it possible that some of the blame (at times) can be shifted to the parents?\",\n       b'Are oxidation reactions exothermic or endothermic? What are some examples of endothermic oxidation reactions?',\n       b'Pain is a subjective sensation produced due to nerve damage. So, does eating spicy food cause possible or impending nerve damage?'],\n      dtype=object)>"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_examples_batch, train_labels_batch = next(iter(train_data.batch(10)))\n",
    "\n",
    "# reshape \n",
    "train_examples_batch = tf.reshape(train_examples_batch, [10,])\n",
    "\n",
    "# batches --> note the shape\n",
    "train_examples_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Build the model\n",
    "\n",
    "three main architecture decisions:\n",
    "1) how to represent the data (the text)\n",
    "2) how many layers to use in the model \n",
    "3) how many **hidden** units to use for each layer \n",
    "\n",
    "Transfer Learning \n",
    "One way to represent the text is to convert sentences into embeddings vectors. We can use a pre-trained text embedding as the first layer, which will have three advantages:\n",
    "\n",
    "1) we don't have to worry about text preprocessing,\n",
    "2) we can benefit from transfer learning,\n",
    "3) the embedding has a fixed size, so it's simpler to process.\n",
    "\n",
    "https://blog.fastforwardlabs.com/2019/09/05/transfer-learning-from-the-ground-up.html\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "For this example we will use a pre-trained text embedding model from TensorFlow Hub called google/tf2-preview/gnews-swivel-20dim/1.\n",
    "\n",
    "There are three other pre-trained models to test for the sake of this tutorial:\n",
    "\n",
    "1) google/tf2-preview/gnews-swivel-20dim-with-oov/1 - same as google/tf2-preview/gnews-swivel-20dim/1, but with 2.5% vocabulary converted to OOV buckets. This can help if vocabulary of the task and vocabulary of the model don't fully overlap.\n",
    "\n",
    "2) google/tf2-preview/nnlm-en-dim50/1 - A much larger model with ~1M vocabulary size and 50 dimensions.\n",
    "\n",
    "3) google/tf2-preview/nnlm-en-dim128/1 - Even larger model with ~1M vocabulary size and 128 dimensions.\n",
    "\n",
    "Let's first create a Keras layer that uses a TensorFlow Hub model to embed the sentences, and try it out on a couple of input examples. \n",
    "\n",
    "Note that no matter the length of the input text, the output shape of the embeddings is: (num_examples, embedding_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "      tf.keras.metrics.TruePositives(name='tp'),\n",
    "      tf.keras.metrics.FalsePositives(name='fp'),\n",
    "      tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "      tf.keras.metrics.FalseNegatives(name='fn'), \n",
    "      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      tf.keras.metrics.Precision(name='precision'),\n",
    "      tf.keras.metrics.Recall(name='recall'),\n",
    "      tf.keras.metrics.AUC(name='auc'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
    "hub_layer = hub.KerasLayer(embedding, input_shape=[], \n",
    "                           dtype=tf.string, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop out layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nkeras_layer_1 (KerasLayer)   (None, 20)                400020    \n_________________________________________________________________\ndense (Dense)                (None, 16)                336       \n_________________________________________________________________\ndense_1 (Dense)              (None, 1)                 17        \n=================================================================\nTotal params: 400,373\nTrainable params: 400,373\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "# building full model \n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "# initialize output bias \n",
    "\n",
    "model.add(hub_layer)\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model compile\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 1/20\n1633/1633 [==============================] - 24s 15ms/step - loss: 0.1685 - accuracy: 0.9378 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\nEpoch 2/20\n1633/1633 [==============================] - 19s 11ms/step - loss: 0.1239 - accuracy: 0.9515 - val_loss: 0.1235 - val_accuracy: 0.9520\nEpoch 3/20\n1633/1633 [==============================] - 21s 13ms/step - loss: 0.1166 - accuracy: 0.9539 - val_loss: 0.1220 - val_accuracy: 0.9527\nEpoch 4/20\n1633/1633 [==============================] - 22s 13ms/step - loss: 0.1127 - accuracy: 0.9553 - val_loss: 0.1217 - val_accuracy: 0.9531\nEpoch 5/20\n1633/1633 [==============================] - 27s 16ms/step - loss: 0.1098 - accuracy: 0.9564 - val_loss: 0.1220 - val_accuracy: 0.9532\nEpoch 6/20\n1633/1633 [==============================] - 20s 12ms/step - loss: 0.1074 - accuracy: 0.9574 - val_loss: 0.1223 - val_accuracy: 0.9532\nEpoch 7/20\n1633/1633 [==============================] - 18s 11ms/step - loss: 0.1051 - accuracy: 0.9584 - val_loss: 0.1234 - val_accuracy: 0.9533\nEpoch 8/20\n1633/1633 [==============================] - 22s 13ms/step - loss: 0.1030 - accuracy: 0.9593 - val_loss: 0.1241 - val_accuracy: 0.9534\nEpoch 9/20\n1633/1633 [==============================] - 23s 14ms/step - loss: 0.1009 - accuracy: 0.9603 - val_loss: 0.1249 - val_accuracy: 0.9531\nEpoch 10/20\n1633/1633 [==============================] - 20s 12ms/step - loss: 0.0988 - accuracy: 0.9610 - val_loss: 0.1264 - val_accuracy: 0.9531\nEpoch 11/20\n1633/1633 [==============================] - 18s 11ms/step - loss: 0.0967 - accuracy: 0.9619 - val_loss: 0.1280 - val_accuracy: 0.9528\nEpoch 12/20\n1633/1633 [==============================] - 20s 12ms/step - loss: 0.0946 - accuracy: 0.9627 - val_loss: 0.1290 - val_accuracy: 0.9526\nEpoch 13/20\n1633/1633 [==============================] - 24s 15ms/step - loss: 0.0925 - accuracy: 0.9637 - val_loss: 0.1307 - val_accuracy: 0.9520\nEpoch 14/20\n1633/1633 [==============================] - 31s 19ms/step - loss: 0.0904 - accuracy: 0.9646 - val_loss: 0.1337 - val_accuracy: 0.9521\nEpoch 15/20\n1633/1633 [==============================] - 19s 12ms/step - loss: 0.0884 - accuracy: 0.9655 - val_loss: 0.1348 - val_accuracy: 0.9515\nEpoch 16/20\n1633/1633 [==============================] - 34s 21ms/step - loss: 0.0865 - accuracy: 0.9663 - val_loss: 0.1366 - val_accuracy: 0.9507\nEpoch 17/20\n1633/1633 [==============================] - 19s 12ms/step - loss: 0.0845 - accuracy: 0.9673 - val_loss: 0.1392 - val_accuracy: 0.9507\nEpoch 18/20\n1633/1633 [==============================] - 19s 12ms/step - loss: 0.0826 - accuracy: 0.9681 - val_loss: 0.1430 - val_accuracy: 0.9506\nEpoch 19/20\n1633/1633 [==============================] - 18s 11ms/step - loss: 0.0807 - accuracy: 0.9688 - val_loss: 0.1444 - val_accuracy: 0.9503\nEpoch 20/20\n1633/1633 [==============================] - 30s 18ms/step - loss: 0.0790 - accuracy: 0.9696 - val_loss: 0.1473 - val_accuracy: 0.9503\n"
    }
   ],
   "source": [
    "# train model\n",
    "history = model.fit(train_data.shuffle(10000).batch(512),\n",
    "                    epochs=20,\n",
    "                    validation_data=validation_data.batch(512),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "511/511 - 2s - loss: 0.1436 - accuracy: 0.9510\nloss: 0.144\naccuracy: 0.951\n"
    }
   ],
   "source": [
    "# evaluate \n",
    "results = model.evaluate(test_data.batch(512), verbose=2)\n",
    "\n",
    "for name, value in zip(model.metrics_names, results):\n",
    "  print(\"%s: %.3f\" % (name, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_validation_split = tfds.Split.TRAIN.subsplit([6, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reshape(train_data,((), ()))"
   ]
  }
 ]
}