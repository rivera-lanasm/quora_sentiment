{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36964bitf2cd31bfb32c4c9ea03831980e0b0932",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "# import tensorflow_datasets as tfds\n",
    "\n",
    "# using this tutorial \n",
    "# https://www.tensorflow.org/tutorials/keras/text_classification_with_hub\n",
    "\n",
    "# uses tf.keras, a high-level API to build and train models in TensorFlow, and TensorFlow Hub, a library and platform for transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Version:  2.0.0\nEager mode:  True\nHub version:  0.7.0\nGPU is NOT AVAILABLE\n"
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# !pip install -q tensorflow-hub\n",
    "# !pip install -q tensorflow-datasets\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"Hub version: \", hub.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bucket = \"quora-questions\"\n",
    "# file_name = \"data/train.csv\"\n",
    "\n",
    "# s3 = boto3.client('s3') \n",
    "# # 's3' is a key word. create connection to S3 using default config and all buckets within S3\n",
    "\n",
    "# obj = s3.get_object(Bucket= bucket, Key= file_name) \n",
    "# # get object and file (key) from bucket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "# load data \n",
    "data = pd.read_csv(\"~/projects/quora_data/train.csv\")\n",
    "data = data[['target', 'question_text']]\n",
    "target = data.pop(\"target\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.33, random_state=42)\n",
    "\n",
    "# load to tf.data --> train\n",
    "train_data = tf.data.Dataset.from_tensor_slices((X_train.values.reshape((875101, )),y_train.values))\n",
    "\n",
    "\n",
    "# load to tf.data --> validation\n",
    "validation_data = tf.data.Dataset.from_tensor_slices((X_test.values.reshape((431021, )), y_test.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=11417, shape=(10,), dtype=string, numpy=\narray([b'What are the differences between the Indian work culture and the UK work culture?',\n       b'Is this English sentence correct and not weird?',\n       b'How much of a \"leg up\" would the Cornell human ecology be for a student? Worth the amount of money it would cost?',\n       b'Will Trump get away with all of his lies and blatantly corrupt behavior?',\n       b'What is the meaning of freedom of speech for RSS & BJP person?',\n       b'What is the altitude of Genting Highlands?',\n       b'Is GPA useful for getting better universities through GRE?',\n       b'Can you compare Pakistani with pigs?',\n       b'How pashmina shawl is formed?', b'How is murder morally wrong?'],\n      dtype=object)>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_examples_batch, train_labels_batch = next(iter(train_data.batch(10)))\n",
    "\n",
    "\n",
    "# reshape \n",
    "train_examples_batch = tf.reshape(train_examples_batch, [10,])\n",
    "\n",
    "# batches --> note the shape\n",
    "train_examples_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Build the model\n",
    "\n",
    "three main architecture decisions:\n",
    "1) how to represent the data (the text)\n",
    "2) how many layers to use in the model \n",
    "3) how many **hidden** units to use for each layer \n",
    "\n",
    "Transfer Learning \n",
    "One way to represent the text is to convert sentences into embeddings vectors. We can use a pre-trained text embedding as the first layer, which will have three advantages:\n",
    "\n",
    "1) we don't have to worry about text preprocessing,\n",
    "2) we can benefit from transfer learning,\n",
    "3) the embedding has a fixed size, so it's simpler to process.\n",
    "\n",
    "https://blog.fastforwardlabs.com/2019/09/05/transfer-learning-from-the-ground-up.html\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "For this example we will use a pre-trained text embedding model from TensorFlow Hub called google/tf2-preview/gnews-swivel-20dim/1.\n",
    "\n",
    "There are three other pre-trained models to test for the sake of this tutorial:\n",
    "\n",
    "1) google/tf2-preview/gnews-swivel-20dim-with-oov/1 - same as google/tf2-preview/gnews-swivel-20dim/1, but with 2.5% vocabulary converted to OOV buckets. This can help if vocabulary of the task and vocabulary of the model don't fully overlap.\n",
    "\n",
    "2) google/tf2-preview/nnlm-en-dim50/1 - A much larger model with ~1M vocabulary size and 50 dimensions.\n",
    "\n",
    "3) google/tf2-preview/nnlm-en-dim128/1 - Even larger model with ~1M vocabulary size and 128 dimensions.\n",
    "\n",
    "Let's first create a Keras layer that uses a TensorFlow Hub model to embed the sentences, and try it out on a couple of input examples. \n",
    "\n",
    "Note that no matter the length of the input text, the output shape of the embeddings is: (num_examples, embedding_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
    "hub_layer = hub.KerasLayer(embedding, input_shape=[], \n",
    "                           dtype=tf.string, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nkeras_layer_1 (KerasLayer)   (None, 20)                400020    \n_________________________________________________________________\ndense_2 (Dense)              (None, 16)                336       \n_________________________________________________________________\ndense_3 (Dense)              (None, 1)                 17        \n=================================================================\nTotal params: 400,373\nTrainable params: 400,373\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "# building full model \n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(hub_layer)\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model compile\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 1/20\n1710/1710 [==============================] - 27s 16ms/step - loss: 0.1564 - accuracy: 0.9431 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\nEpoch 2/20\n1710/1710 [==============================] - 24s 14ms/step - loss: 0.1215 - accuracy: 0.9523 - val_loss: 0.1206 - val_accuracy: 0.9533\nEpoch 3/20\n1710/1710 [==============================] - 23s 13ms/step - loss: 0.1158 - accuracy: 0.9542 - val_loss: 0.1198 - val_accuracy: 0.9538\nEpoch 4/20\n1710/1710 [==============================] - 23s 14ms/step - loss: 0.1123 - accuracy: 0.9553 - val_loss: 0.1194 - val_accuracy: 0.9540\nEpoch 5/20\n1710/1710 [==============================] - 22s 13ms/step - loss: 0.1097 - accuracy: 0.9563 - val_loss: 0.1209 - val_accuracy: 0.9539\nEpoch 6/20\n1710/1710 [==============================] - 23s 13ms/step - loss: 0.1074 - accuracy: 0.9572 - val_loss: 0.1204 - val_accuracy: 0.9540\nEpoch 7/20\n1710/1710 [==============================] - 23s 13ms/step - loss: 0.1053 - accuracy: 0.9581 - val_loss: 0.1220 - val_accuracy: 0.9539\nEpoch 8/20\n1710/1710 [==============================] - 23s 13ms/step - loss: 0.1033 - accuracy: 0.9589 - val_loss: 0.1222 - val_accuracy: 0.9539\nEpoch 9/20\n1710/1710 [==============================] - 24s 14ms/step - loss: 0.1014 - accuracy: 0.9597 - val_loss: 0.1238 - val_accuracy: 0.9540\nEpoch 10/20\n1710/1710 [==============================] - 23s 14ms/step - loss: 0.0995 - accuracy: 0.9605 - val_loss: 0.1247 - val_accuracy: 0.9536\nEpoch 11/20\n1710/1710 [==============================] - 23s 14ms/step - loss: 0.0976 - accuracy: 0.9614 - val_loss: 0.1259 - val_accuracy: 0.9533\nEpoch 12/20\n1710/1710 [==============================] - 23s 14ms/step - loss: 0.0958 - accuracy: 0.9621 - val_loss: 0.1277 - val_accuracy: 0.9535\nEpoch 13/20\n1710/1710 [==============================] - 26s 15ms/step - loss: 0.0939 - accuracy: 0.9629 - val_loss: 0.1289 - val_accuracy: 0.9531\nEpoch 14/20\n1710/1710 [==============================] - 25s 15ms/step - loss: 0.0921 - accuracy: 0.9637 - val_loss: 0.1306 - val_accuracy: 0.9530\nEpoch 15/20\n1710/1710 [==============================] - 22s 13ms/step - loss: 0.0903 - accuracy: 0.9645 - val_loss: 0.1330 - val_accuracy: 0.9530\nEpoch 16/20\n1710/1710 [==============================] - 21s 12ms/step - loss: 0.0885 - accuracy: 0.9653 - val_loss: 0.1347 - val_accuracy: 0.9527\nEpoch 17/20\n1710/1710 [==============================] - 20s 12ms/step - loss: 0.0867 - accuracy: 0.9660 - val_loss: 0.1364 - val_accuracy: 0.9520\nEpoch 18/20\n1710/1710 [==============================] - 21s 12ms/step - loss: 0.0850 - accuracy: 0.9668 - val_loss: 0.1385 - val_accuracy: 0.9519\nEpoch 19/20\n1710/1710 [==============================] - 21s 12ms/step - loss: 0.0832 - accuracy: 0.9676 - val_loss: 0.1409 - val_accuracy: 0.9518\nEpoch 20/20\n1710/1710 [==============================] - 21s 12ms/step - loss: 0.0816 - accuracy: 0.9683 - val_loss: 0.1427 - val_accuracy: 0.9510\n"
    }
   ],
   "source": [
    "# train model\n",
    "history = model.fit(train_data.shuffle(10000).batch(512),\n",
    "                    epochs=20,\n",
    "                    validation_data=validation_data.batch(512),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-f46b61d2dca1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s: %.3f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_data' is not defined"
     ]
    }
   ],
   "source": [
    "# evaluate \n",
    "results = model.evaluate(test_data.batch(512), verbose=2)\n",
    "\n",
    "for name, value in zip(model.metrics_names, results):\n",
    "  print(\"%s: %.3f\" % (name, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_validation_split = tfds.Split.TRAIN.subsplit([6, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reshape(train_data,((), ()))"
   ]
  }
 ]
}