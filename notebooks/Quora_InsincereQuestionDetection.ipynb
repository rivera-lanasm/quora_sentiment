{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Version:  2.0.0\n"
     ]
    }
   ],
   "source": [
    "# set working directory to parent\n",
    "import os\n",
    "os.chdir(\"..\")\n",
    "\n",
    "# import packages\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import numpy as np\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from src.data import process_data \n",
    "\n",
    "# check tf version\n",
    "print(\"TF Version: \", tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quora Insinceere Question Classification \n",
    "\n",
    "[Kaggle Competition](https://www.kaggle.com/c/quora-insincere-questions-classification/notebooks)\n",
    "\n",
    "This project is meant to help me explore some of the theory behind neural network models, as well as the methodologies behind designing their architectures and training them. \n",
    "\n",
    "Here is an excerpt from the problem statement from the competition site:\n",
    "\"An existential problem for any major website today is how to handle toxic and divisive content. Quora wants to tackle this problem head-on to keep their platform a place where users can feel safe sharing their knowledge with the world...**A key challenge is to weed out insincere questions -- those founded upon false premises, or that intend to make a statement rather than look for helpful answers**... Help Quora uphold their policy of “Be Nice, Be Respectful” and continue to be a place for sharing and growing the world’s knowledge.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approaches\n",
    "\n",
    "All of the approaches in this notebook will involve neural networks written in the Tensorflow 2 framework. Below is a list of approaches and methodologies that will be tested across the different models I implement, roughly in the order in which they appear in the pipeline. \n",
    "\n",
    "1. Train-Test Set Splitting\n",
    "  - Stratified Sampling\n",
    "2. Data Preprocessing\n",
    "  - Embedding Layers/Transfer Learning\n",
    "  - Oversampling methods (SMOTE, ADASYN, Random oversampling)\n",
    "  - \n",
    "3. NN Model Cost functions & Evaluation \n",
    "2. NN Model Architecture\n",
    "3. NN Model Hyperparameters\n",
    "4. NN Training Strategies\n",
    "5. Overfitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approaching Imbalanced Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples:\n",
      "    Total: 1306122\n",
      "    Positive: 80810 (6.19% of total)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = process_data.retrieve_training(bucket = \"quora-questions\", file_name = \"data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'process_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1a6d784558bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"quora-questions\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"data/train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'process_data' is not defined"
     ]
    }
   ],
   "source": [
    "train_data,test_data = process_data.train_split(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Methodology\n",
    "The competition states that submissions will be evaluated upon F1 Score between predicted and observed targets \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test.csv     \n",
    "\n",
    "test_data = process_data.retrieve_test(bucket = \"quora-questions\", file_name = \"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"mriv_model0_exp0\"\n",
    "filepath = os.getcwd()+\"/data/saved_models/{}.h5\".format(model)\n",
    "\n",
    "loaded_model = tf.keras.models.load_model(filepath,custom_objects={'KerasLayer':hub.KerasLayer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_batch = loaded_model.predict(test_data['question_text'].values)\n",
    "predicted_id = np.argmax(predicted_batch, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg, pos = np.bincount(predicted_id)\n",
    "total = neg + pos\n",
    "print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(\n",
    "        total, pos, 100 * pos / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\"qid\":test_data['qid'].values, 'prediction':predicted_id})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission_test.csv\")"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('quora': conda)",
   "language": "python",
   "name": "python361064bitquoraconda9b0626ecaedd4d86b27b6bf9323516ef"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
